---
title: "Data Pipeline"
author: "Lee Durbin"
format: html
---

## Problem Description

This project is the final requirement for the [Data Engineering Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp).

In this project, we're going to evaluate the popularity of the [plotnine](https://plotnine.org/) Python library over the last two years. *plotnine* is an implementation of a *grammar of graphics* in Python based on ggplot2. I'm an enthuasiastic user of the R programming language on which [ggplot2](https://ggplot2.tidyverse.org/) is built, so it seems fitting for me to evaluate a Python package based on an R package.

The data about *plotnine* download comes from a public dataset hosted on Google BigQuery, which provides download stats on all the packages hosted on PyPI. I got the idea for using this dataset after watching a [End-to-End Data Engineering tutorial on YouTube](https://www.youtube.com/watch?v=3pLKTmdWDXk) which uses this same dataset to analyse how the duckdb Python package has been performing. I've also found [this blog post](https://www.r-bloggers.com/2023/12/data-engineering-in-r-how-to-build-your-first-data-pipeline-with-r-mage-and-google-cloud-platform-in-under-45-minutes/) quite helpful. And, of course, everything I've learned in the course so far!

I'm going to attempt to build as much of the data pipeline as possible using R, and these instructions were written using Quarto in RStudio.

The GitHub repo for this project can be found [here](https://github.com/lddurbin/data-eng-project).

## Getting Started

If you want to follow along, start by running `renv::restore()` after cloning this repo to make sure you have the same packages as me.

Let's load the packages we'll need:

```{r}
#| output: false

library(googleCloudStorageR)
library(janeaustenr)
library(tidytext)
library(dplyr)
library(googleComputeEngineR)
library(bigrquery)
```


I'm assuming that you have a Google Cloud account, you've created a new project, and you've created a service account with the permissions described in [this video](https://youtu.be/Y2ux7gq3Z0o&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=12). I also assume you've downloaded your key as a JSON file, and you're pointing to that file in a .Renviron file in this project as follows:

```{r service_key}
#| eval: false

GCS_AUTH_FILE = "/path/to/key-file-name.json"
```

Let's configure the settings for our Google Project. We'll use the [googleCloudStorageR](https://code.markedmondson.me/googleCloudStorageR/articles/googleCloudStorageR.html) package for this, which will guide us through the process. The Data Engineering Zoomcamp uses Terraform, but in the interests of time I've opted for a R package instead to achieve the same ends.

Assuming you have the .Renviron file populated as described above, just run the following:

```{r gcp_setup}
#| eval: false

gcs_setup()
```

## Infrastructure as Code

Let's create a new bucket where we'll store our data. We'll need to make we're using a unique name for the new bucket, so to have some fun let's grab a couple of words from the works of Jane Austen, following along with [Julie Silge's instructions](https://github.com/juliasilge/tidytext) for her tidytext package:

```{r austen_words}

austen_words <- austen_books() |> 
  mutate(line = row_number(), .by = "book") |> 
  unnest_tokens(word, text) |> 
  anti_join(get_stopwords())

bucket_words <- austen_words |> 
  pull(word) |> 
  unique() |> 
  sample(2)

bucket_numbers <- round(runif(1, 1, 10000))

bucket_name <- paste0(bucket_words[1], "_", bucket_words[2], bucket_numbers)

bucket_name

```

Now we can create the bucket. Notice that I'm calling a couple of environment variables again below - my GCP project ID and my the zone where I want to host the bucket, both of which are stored in the .Renviron file we created earlier. You can see how I set these at the top of the following code chunk, followed by the call to GCP:

```{r create_bucket}
# Sys.setenv("GCS_DEFAULT_PROJECT_ID", "your-projectid-1234")
# Sys.setenv("GCS_DEFAULT_LOCATION", "your_nearest_location")

gcs_create_bucket(
  name = bucket_name,
  projectId = Sys.getenv("GCS_DEFAULT_PROJECT_ID"),
  location = Sys.getenv("GCS_DEFAULT_LOCATION"),
  storageClass = c("MULTI_REGIONAL"),
  predefinedAcl = c("projectPrivate"),
  predefinedDefaultObjectAcl = c("bucketOwnerFullControl"),
  projection = c("noAcl")
)

```

If all went well, you should see some text in the console which will confirm that the bucket creation was successful.

We'll now set this bucket name as the default, to use again later when we're uploading to it:

```{r gcs_default_bucket}
gcs_global_bucket(bucket_name)
```

Next up, let's set up some environment variables for Google Compute Engine, then we'll use the googleComputeEngineR package to create a VM as follows:

```{r}
#| eval: false

# Sys.setenv("GCE_AUTH_FILE", "/path/to/key-file-name.json")
# Sys.setenv("GCE_DEFAULT_PROJECT_ID", "your-projectid-1234")
# Sys.setenv("GCE_DEFAULT_ZONE", "your_nearest_zone")


vm <- gce_vm(name = "test-vm", predefined_type = "f1-micro", 
             image_project = "ubuntu-os-cloud", 
             image_family = "ubuntu-2310-amd64")

## VM metadata
vm
```

## Data Ingestion

Ideally the following steps would be handled in Mage, but in the interest of time we're just going to lay it out in this Quarto doc. If time permits, we'll migrate this code into Mage so that the process can be orchestrated.

We're going to query the data about plotnine from the BigQuery public dataset called *pypi*, and then save this to our GCP bucket. First we'll need to point the `bigrquery` package to our credentials in the JSON file via the environment variable in .Renviron:

```{r auth_bigrquery}
bigrquery::bq_auth(path = Sys.getenv("GCS_AUTH_FILE"))
```

And now we can get the PyPI data about plotnine from yesterday, limiting the columns to the timestamp, country_code, and url:

```{r get_bigquery_data}
yesterday <- Sys.Date() -1
data_src <- "`bigquery-public-data.pypi.file_downloads`"

sql <- paste0("SELECT timestamp, country_code, url FROM ",
              data_src,
              " WHERE TIMESTAMP_TRUNC(timestamp, DAY) = TIMESTAMP(\"",
              yesterday,
              "\") AND project = 'plotnine'")

tb <- bq_project_query(Sys.getenv("GCS_DEFAULT_PROJECT_ID"), sql)

plotnine_stats <- bq_table_download(tb)

bq_table_download(tb, n_max = 10)
```

Now let's upload this data as a CSV file into our newly-created GCP bucket, and we'll use the date of the data to determine the filename for it:

```{r upload_to_gcs}
filename <- paste0("plotnine-stats-", as.character(yesterday))

gcs_upload(plotnine_stats, name = filename)
```


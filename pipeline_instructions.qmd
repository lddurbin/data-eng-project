---
title: "Data Pipeline"
author: "Lee Durbin"
format:
  html:
    toc: true
---

## Problem Description

This project is the final requirement for the [Data Engineering Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp).

In this project, we're going to evaluate the popularity of two Python libraries over the last year: [Polars](https://pola.rs/) and [pandas](https://pandas.pydata.org/).

*Polars* is a blazingly fast DataFrame library for manipulating structured data. The core is written in Rust, and available for Python, R and NodeJS. For this project, we're going to evaluate the popularity of its implementation in Python.

*pandas* is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language. 

*pandas* was used in the Data Engineering Zoomcamp, but I found its ergonomics to be sub-optimal compared to R's [dplyr](https://dplyr.tidyverse.org/) package. I recently read an excellent [blogpost by Emily Riederer](https://www.emilyriederer.com/post/py-rgo/), in which she recommends *Polars* as it "may feel more natural and less error-prone to R users for may reasons".

Polars is a relative newcomer, having been founded as an open source project in 2020, whereas pandas has been around for about as long as dplyr. I was curious how *Polars* has been performing over the last year, and how its growth compares to *pandas*.

The data about these libraries comes from a public dataset hosted on Google BigQuery, which provides download stats on all the packages hosted on [PyPI](https://pypi.org/). I got the idea for using this dataset after watching a [End-to-End Data Engineering tutorial on YouTube](https://www.youtube.com/watch?v=3pLKTmdWDXk) which uses this same dataset to analyse how the duckdb Python package has been performing. I've also found [this blog post](https://www.r-bloggers.com/2023/12/data-engineering-in-r-how-to-build-your-first-data-pipeline-with-r-mage-and-google-cloud-platform-in-under-45-minutes/) quite helpful. And, of course, everything I've learned in the course so far!

The GitHub repo for this project can be found [here](https://github.com/lddurbin/data-eng-project).


## Limitations

Over the last couple of months I've had COVID and been challenged by my mental health. This has made it difficult to commit as much to the course as I would have liked to, but I'm still determined to complete this assignment on time. To achieve this whilst still putting my wellbeing first, I'm going to use R to build as much of the data pipeline as possible. Even though the course uses a variety of tools and it would be better to utilise them for this assignment, I have been writing R code most days for the past 4 years so I'm more likely to complete this assignment in time (and with greater ease) by using R. If I have a second shot at this, I will re-build the pipeline using Mage and dbt, and use Terraform for the IaC section.


## Getting Started

If you want to follow along, start by running `renv::restore()` after cloning this repo to make sure you have the same packages as me.

I'm assuming that you have a Google Cloud account, you've created a new project, and you've created a service account with the permissions described in [this video](https://youtu.be/Y2ux7gq3Z0o&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=12). I also assume you've downloaded your key as a JSON file, and you're pointing to that file in a .Renviron file in this project.

There's actually a bunch of environment variables we'll need to set up for this pipeline to run. The `usethis` package makes it easy to edit .Renviron files - just call `usethis::edit_r_environ(scope="project")` and it will take you to a newly-generated .Renviron file. Populate it with something similar to this:

```{r environment_variables}
#| eval: false

GCS_AUTH_FILE = "path/to/creds.json"
GCS_DEFAULT_LOCATION = "<nearest-location>"
GCS_DEFAULT_PROJECT_ID = "<your-project-id>"

GCE_AUTH_FILE = "path/to/creds.json"
GCE_DEFAULT_PROJECT_ID = "<your-project-id>"
GCE_DEFAULT_ZONE = "<nearest-zone>"
```

Now let's load the packages (the ones that will access the Google API will read the newly-created .Renviron file):

```{r load_libraries}
#| output: false
library(usethis)
library(googleCloudStorageR)
library(janeaustenr)
library(tidytext)
library(dplyr)
library(googleComputeEngineR)
library(bigrquery)
```

The first set of variables above will configure the settings for accessing Google Cloud Storage, and we're using the [googleCloudStorageR](https://code.markedmondson.me/googleCloudStorageR/articles/googleCloudStorageR.html) package for this. The Data Engineering Zoomcamp uses Terraform, but in the interests of time I've opted for a R package instead to achieve the same ends.


## Infrastructure as Code

Let's create a new bucket where we'll store our data. We'll need to make we're using a unique name for the new bucket, so to have some fun let's grab a couple of words from the works of Jane Austen. [Julie Silge's instructions](https://github.com/juliasilge/tidytext) for her tidytext package really helped me here:

```{r austen_words}

austen_words <- austen_books() |> 
  unnest_tokens(word, text) |> 
  anti_join(get_stopwords())

bucket_words <- austen_words |> 
  pull(word) |> 
  unique() |> 
  sample(2)

bucket_numbers <- round(runif(1, 1, 10000))

bucket_name <- paste0(bucket_words[1], "_", bucket_words[2], bucket_numbers)

bucket_name

```

Now we can create the bucket. Notice that I'm calling a couple of environment variables below - my GCP project ID and my the zone where I want to host the bucket, both of which are stored in the .Renviron file we created earlier.

```{r create_bucket}

gcs_create_bucket(
  name = bucket_name,
  projectId = Sys.getenv("GCS_DEFAULT_PROJECT_ID"),
  location = Sys.getenv("GCS_DEFAULT_LOCATION"),
  storageClass = c("MULTI_REGIONAL"),
  predefinedAcl = c("projectPrivate"),
  predefinedDefaultObjectAcl = c("bucketOwnerFullControl"),
  projection = c("noAcl")
)

```

If all went well, you should see some text in the console which will confirm that the bucket creation was successful.

We'll now set this bucket name as the default, to use again later when we're uploading to it:

```{r gcs_default_bucket}
gcs_global_bucket(bucket_name)
```

Next up, let's use the googleComputeEngineR package to create a VM as follows:

```{r}
#| eval: false

vm <- gce_vm(name = "test-vm", predefined_type = "f1-micro", 
             image_project = "ubuntu-os-cloud", 
             image_family = "ubuntu-2310-amd64")

## VM metadata
vm
```

## Data Ingestion

Ideally the following steps would be handled in Mage, but in the interest of time we're just going to lay it out in this Quarto doc. If time permits, we'll migrate this code into Mage so that the process can be orchestrated.

We're going to query the data about pandas and Polars from the BigQuery public dataset called *pypi*, and then save this to our GCP bucket as CSV files. First we'll need to point the [bigrquery](https://bigrquery.r-dbi.org/index.html) package to our credentials in the JSON file via the environment variable in .Renviron:

```{r auth_bigrquery}
bigrquery::bq_auth(path = Sys.getenv("GCS_AUTH_FILE"))
```

And now we can query the PyPI data to get the number of downloads of pandas and Polars from the last year:

```{r get_bigquery_data}

library(DBI)

query_tbl <- function(data, lib_name) {
  last_year <- Sys.Date() - 365
  
  lib_stats <- data |> 
  filter(
    project == lib_name,
    as.Date(timestamp) >= last_year
  ) |> 
  mutate(date = as.Date(timestamp)) |> 
  count(date) |> 
  collect()
  
  return(lib_stats)
}

con <- dbConnect(
  bigrquery::bigquery(),
  project = "bigquery-public-data",
  dataset = "pypi",
  billing = Sys.getenv("GCS_DEFAULT_PROJECT_ID")
)

downloads <- tbl(con, "file_downloads")

polars_stats <- query_tbl(downloads, "polars")
pandas_stats <- query_tbl(downloads, "pandas")

dbDisconnect(con)
```

Now let's upload this data as CSV files into our newly-created GCP bucket, and we'll use the date of the data to determine the filename for it:

```{r upload_to_gcs}

pandas_filename <- paste0("pandas-stats-", min(pandas_stats$date), "_", 
                          max(pandas_stats$date))

polars_filename <- paste0("polars-stats-", min(polars_stats$date), "_", 
                          max(polars_stats$date))

gcs_upload(pandas_stats, name = pandas_filename)
gcs_upload(polars_stats, name = polars_filename)
```


## Data Warehouse

Now let's create a BigQuery dataset if we don't already have it:

```{r bq_dataset}
pypi <- bq_dataset(Sys.getenv("GCS_DEFAULT_PROJECT_ID"), "pypi")
dataset_exists <- bq_dataset_exists(pypi)

if(dataset_exists == FALSE) {
  bq_dataset_create(pypi)
}
```

Let's combine our dataframes into one, making sure we tag each first:

```{r combine_dataframes}
pandas_tagged <- pandas_stats |> 
  mutate(library = "pandas")

polars_tagged <- polars_stats |> 
  mutate(library = "polars")

python_lib_stats <- bind_rows(pandas_tagged, polars_tagged)

```


Next, let's create a BigQuery table if we don't already have it:

```{r bq_table}
stats_tbl <- bq_table(pypi, "python_lib_stats")
table_exists <- bq_table_exists(stats_tbl)

if(table_exists == FALSE) {
  fields <- as_bq_fields(python_lib_stats)
  
  bq_table_create(stats_tbl, fields)
}
```

Now we can load this into our BigQuery table:

```{r bq_upload}
bq_table <- bq_table(Sys.getenv("GCS_DEFAULT_PROJECT_ID"), "pypi", 
                     "python_lib_stats", type = "TABLE")

bq_table_upload(bq_table, python_lib_stats)
```

